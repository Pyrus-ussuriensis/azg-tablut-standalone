# Tablut
## understand project and add annotations
### main stream
从mainTafl开始，首先创建一个Game实例，其来自TaflGame，继承了Game。
{
    TaflGame在初始化时会先获得一个棋盘，棋盘来自TaflLogic，其通过指定了实际的选择的棋种的类确定了关于棋的参数。
    [
        在棋盘Board内部也会维护棋盘的基本参数包括宽高，棋盘状态，棋子状态，进行轮数和是否终局状态。其提供了如下功能：**复制棋盘**，这里需要先获得空白棋盘，空白棋盘来自GameVariants。       
        (
            其会根据你指定的变种制作出1/8的棋盘，然后逐步复制得到完整的棋盘类型。
        )
        然后将自己的各个属性复制给这个白板；**数出给定类型的棋子数量**，首先在所有棋子中选出活子，死子的x坐标是-99，然后看颜色，一致则计数；**获得合法动作集**，对于自己的每个子，依次检测位置是否越位，是否活着，是否沿直线运动，是否动了，是否动的是自己的棋子，目的地是否是禁落点，路径中途是否有其他棋子，同时根据不同的错误返回不同的错误值；**是否有合法动作**，检测合法动作集的长度；**执行动作**，获得位置的棋子的对应编号，检测动作是否合法，同样是上面的步骤，如果合法则执行动作，执行时先检查是否是合法动作，轮数增加一轮，然后获取杀死的棋子，对于所有子检测是否是对手的子，同时检测是否和自己紧挨着，然后检测另一边是自己的子，如果是则记录，然后将所有杀死的子的x轴值变成-99，然后检测此时状态，首先看是否轮数足够多，避免拖延，如果足够多白输，然后遍历所有子找王，看是否活着如果死了白输，如果活着看是否在逃出的点，如果在则白胜，如果不在则继续；**获取当前的玩家**，根据当前的轮数判断；还有其他功能如切片，索引，可视化棋盘之类
    ]
    其提供了如下功能：**获取棋盘尺寸**，会维护，初始从棋盘读取；**获取动作空间**，棋盘尺寸四次方，默认是从任意位置到任意位置；**获取下一个状态**，根据当前状态输入棋盘做执行得到反馈返回，期间使用了int2base，来自Digits，其作用是将动作空间的单一数值转化为实际的动作，即四个值；**获取合法动作掩码**，建立空的掩码，然后读取合法动作，将合法的标记为1，这里涉及到从四值动作到单一值的转换；**获取终局状态**，如果没有结束为0，如果结束了乘当前玩家获取自己视角的输赢结果，1或-1；**规范表达**，在一些对称化游戏只要翻转一下就可以重新利用，但是这里不行，因为双方的目标不同；**获取对称**，这里没有实现；**获取分数**，如果结束，获取自己视角分数乘1000倍，如果没有结束则返回目前自己剩余的子数。同文件中还提供了一个**打印棋盘**的功能。
}
然后我们会获取网络，我们输入Game，到NNetWrapper
{
    其维护网络TaflNNet
    [
        网络根据棋盘内容输入输出各个动作的概率和当前状态的值
    ]
    维护棋盘的基本状态，宽高，动作空间大小。提供如下功能：**训练**，输入数据，包括输入内容，MCTS的两个输出，然后获取网络的当前输入计算和MCTS输出的损失，优化，记录；**预测**，给棋盘输入，输入网络得到两个输出；**两个损失的计算函数**；**保存和加载网络的权重**。
}
得到网络后我们先看是否要加载权重。然后输入得到Coach对象，
{
    其维护游戏，两个网络，MCTS功能
    [
        树要记录游戏和网络，树的各个值，然后提供如下功能：**提供动作概率的估计**，先搜索一定次数，然后根据温度，如果为0则纯贪心选择次数最大的，如果不是则根据温度调整输出压平；**搜索**，整个流程是先获取要计算u需要的值，如果不存在的补一个初值，像当前状态的价值和各个动作初值直接用网络预测输出获取，然后计算出所有动作的u值，然后得到最大的，再选择值最大的动作，给环境跑这个动作，递归的跑，然后得到返回值后更新相应的边的值，知道此局结束后返回。
    ]
    训练历史的记录，提供如下功能：**提供训练数据**，跑一个episode，先初始化棋盘，轮数，玩家，每一步增加轮数，此轮数用于调节温度而不是调节time，从MCTS获取建议，将当前棋盘和建议打包成历史训练数据，从建议中选择抽样选择动作，跑一步，获取终局状态，如果终局则整理记录的数据返回；**学习**，学习参数指定的轮数，每一轮跑若干episode，先重置MCTS树，然后开始跑获取数据，然后记录到总的历史维护变量中，总的维护有定数，超了去除旧数据，保存数据到磁盘，打乱数据，然后加载网络，两个网络共享一份权重，新网络开始训练，训练完成后开始生成一个Arena进行测试，
    [
        其维护棋手，游戏和显示，提供功能：**跑一局**，初始化棋盘和轮数，当前玩家，每一步先检测游戏是否结束，然后轮数加一，用玩家获取动作，检查是否合法，获取游戏下一状态，然后最终结果；**跑一轮测试**，根据给定的轮数，跑一半，交换棋子，跑另一半，记录输赢平，返回。
    ]
    其以lambda函数为输入，此函数以棋盘为输入，自动调用MCTS求取动作概率分布函数，贪心得到动作。跑一轮测试，如果全平或者胜的占胜负和的比例没有达到阈值则不会使用新权重，让新网络重新加载旧权重，如果超过了则根据这一轮序号记录这一轮权重，同时记录当前最佳权重；除此以外还提供了**保存加载训练数据**，**获取权重文件名**的函数。
}
让Coach开始学习。

### compare players
在pitTafl中先得到游戏和三个玩家，来自TaflPlayers分别提供不同的策略，简单的通过play提供，加载一个Arena，让任意两位进行游玩，双方各执一次黑白。

### problems
当前代码相较于我的目标存在如下问题：
1. 禁落点处规则和我希望的有差异，禁落点任何棋子不可经过，同时王在离开中心禁落点后不可返回，这个规则需要修改。
2. 胜利规则此处实现的是到达四角处胜利，我希望的是到达边沿处胜利。
3. 没有实现镜像的数据增强。
4. 对于网络的效率评估停留在相互之间的对比而缺少一个稳定的可量化的性能分析，应补充一个，比如以alphabeta树为固定Elo分数的固定基准。
5. 代码存在问题
   1. 首次运行暴露问题
      1. predict对于输入的board做了格式转换，而train中没有，从而报错。位于NNet.py
      2. MCTS的Search中使用E记录各个状态的终局值，但是如果相同的状态之前没有终局，但之后因为超时终局则会陷入死循环，位于MCTS.py
   2. AI发现问题
      1. _isLegalMove的越界判定问题
      2. int2base的处理在0处等

## fix game rules problems
### the rules in Bannerlord
这个规则和我找到的Tablut文件中的规则有区别，更加简化，比如没有王在王座上需要四面围杀，在王座旁需要三面围杀等规则。
1. 9×9 棋盘，黑方（进攻方）16子对白方（防守方）8子+王从中宫起步；防守方把王走到任意边格即胜，进攻方吃掉王或让所有防守方无子可动即胜。
2. 所有棋子（包括王）直线走任意格，不可跳子；任何棋子都不能走到或穿过中心格（王座）。王一旦离开中心格，也不能再跨越它。
3. 基本吃法是两侧夹击。此外，可以把敌子“顶在中心格上”吃掉——也就是把对方夹在己子与中心格之间（这一点双方都适用）。
4. 王在王座上仍然按普通两侧夹杀被吃（并没有“必须四面合围”的特别条款）。
5. 王不在王座上（王座为空）：可以。黑子可把白兵夹在“己子 + 空王座”之间而吃掉。王仍在王座上：不能把王座当作“墙”参与夹杀旁兵。

## data segmetation
这里的数据增强是通过对数据旋转和镜像实现。具体而言对于np数组可以用函数对其方便的旋转和镜像，而对于一般的pi因为是四个坐标值，所以难以使用，所以要手动一对对的翻转。但是实时的慢，可以制作一次表，之后直接按照对应位置映射。但训练时间增加8倍，包括旋转4个和翻转2个，后续可能考虑调整训练网络时数据的取用。
现在修改为不在制作数据时做数据增强，而是在训练前取数据时随机的选择一种转换方式。这样减少了前期的时间开销，同时保证了训练时各个数据增强在统计意义上的效果。

## game rules test
利用pytest进行简单的情况的测试
